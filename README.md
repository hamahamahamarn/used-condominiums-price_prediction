# データについて
[データの取得はこちらから行えます](https://competition.nishika.com/competitions/mansion_2024spring/data)
データ数：820017, 特徴量の数：28
- ID
- 種類
- 地域
- 市区町村コード
- 都道府県名
- 市区町村名
- 地区名
- 最寄駅：名称
- 最寄駅：距離（分）
- 間取り
- 面積（㎡）
- 土地の形状
- 間口
- 延床面積（㎡）
- 建築年
- 建物の構造
- 用途
- 今後の利用目的
- 前面道路：方位 
- 前面道路：種類 
- 前面道路：幅員（ｍ）
- 都市計画
- 建ぺい率（％）
- 容積率（％）
- 取引時点
- 改装
- 取引の事情等
- 取引価格（総額）_log `目的変数`

# EDA+前処理、特徴量エンジニアリング
- df.nuniqueの結果を踏まえ、種類が0or1であった以下の特徴量を削除する。
  - 市区町村コード
  - 種類
  - 地域
  - 土地の形状
  - 間口
  - 延床面積（㎡）
  - 前面道路：方位
  - 前面道路：種類
  - 前面道路：幅員（ｍ）

- object型のもので数値化できる特徴量（表記ゆれの訂正）  
["最寄駅：距離（分）", "間取り", "面積（㎡）", "建築年"] を変換する

- 間取りについては、広いほど価格が上がりそうと予想したがそうでもないことが判明したため、そのままにする


## 欠損値の補完(空白は欠損値とみなされていないため注意が必要)

### 欠損値をすべて 'missing' で補完する場合

もし欠損値処理をするなら・・・  
そのまま（意味をなさないもので補完する）・・・地区名、最寄駅：名称、改装、取引の事情等  
最頻値補完・・・間取り、建物の構造、用途、今後の利用目的、都市計画  
中央値補完 or 他の特徴量を用いて予測して補完・・・最寄駅：距離（分）、建築年、建ぺい率（％）、容積率（％）


### 他の特徴量を用いて予測し、数値の欠損値を補完する場合
最寄駅：距離（分）、建築年、建ぺい率（％）、容積率（％）のそれぞれの欠損値を他の特徴量を用いて予測する

## 新たに追加した特徴量
 1. `取引時点_何年前`：現在から取引時点の差を計算
 2. `面積（㎡）容積率（％）_combi`：面積（㎡）* 容積率（％）/ 100
 3. `取引時点_enc`：「取引時点」カラムをエンコードして、各ユニークな取引時点を整数に変換する処理
 4. `["最寄駅：距離（分）", "面積（㎡）", "建ぺい率（％）", "容積率（％）"]と["count", "mean", "min", "max"]を作用させた特徴量`：時間的な特徴と地域（都道府県名）に基づいたターゲット変数の過去のカウント、平均値、最小値、最大値を計算
 5. `築年数`：建築年から築年数を算出する
 6. `取引時点_日付`：四半期の始まりを示す日付を生成
 7. `消費税率`：当時の消費税率を表す
 8. `madori_mean`：間取りごとの取引価格（総額）_logの平均を取った特徴量を作成する
 9. `最寄り駅乗降客数`：外部データを用いて`最寄駅：名称`に紐づける形で追加
 10. `緯度`：外部データを用いて住所から算出
 11. `経度`：同様に外部データから算出
 12. `東京都港区からの距離`：上記で追加した`緯度`,`経度`から各地点と東京都港区までの距離を算出  
 ＊9~12の特徴量はbase2.csvに追加済み

## カテゴリカル変数の数値化

### 全ての特徴量をダミー化した場合 -> 実行時間の関係で不可能

### 全ての特徴量をラベルエンコーディングした場合
->関係ない順序性が生まれてしまうためか精度はカウントエンコーディングの方が良かった。

### 全ての特徴量をカウントエンコーディングした場合
-> 今回は特徴量ごとの種類の数が直接的に価格に影響を及ぼしそうなのでこちらを利用する。

### 全ての特徴量をターゲットエンコーディングをした場合
-> 過学習してしまった。

# 価格予測
下記の2パターンで予測値を算出した。  
* 単純に`取引価格（総額）`を予測
* `取引価格（総額）`を以下のような変換方法をすることで`取引価格（総額）_per_面積（㎡）_log`として予測  
  1. 目的変数の常用対数を元に戻す
  2. `面積`で割る
  3. 常用対数をとる

## lightgbm
今回は主にこちらのモデルを利用した。
- バリデーション方法
  - hold-out法（`都道府県名`で層化）
  - StratifiedKHold（K=10、同じく`都道府県名`で層化）
- optunaによるハイパーパラメータの調整
- 過学習しすぎない程度に学習回数を増やした。(20000~50000)
- 学習率は学習回数の関係であまり低くできなかった。

## catboost
lightgbmとバリデーション、ハイパーパラメータの調整は同様。
リーダーボードとの乖離が大きかった。

## xgboost
あまり良い精度が出なかったため、使用を見送る。

## ランダムフォレスト
実行時間がかかりすぎるため、利用せず。

## Kerasを使用したディープニューラルネットワーク
データ数が多いため、私のパソコンのマシンパワーだと厳しい、、、  
まず、標準化すると精度が悪化する。

## MLPRegressor
同様にデータ数が多いため、私のパソコンのマシンパワーだと厳しいところがあった。

## pycaret
分析は自分の力でやりたいので精度の良さそうなモデルを見つける際に利用しました。

## スタッキング
-> 実行時間の関係と単純に複数の予測値の平均をとるアンサンブルをした方が精度が良いと考えたため、やめておく

# 予測ファイル提出
層化k分割交差検証の場合は平均値をとって予測値を算出した。

- 住宅価格の補正を行う場合  
最近の住宅価格の高騰を考慮したほうが良いと考え、1.5%, 3.0%, 5.0%と3パターンで試しに補正を加えてみた。  
精度はあまり変わらなかった。


* アンサンブルして予測値を出す。  
2パターンの目的変数で予測したもの  
ハイパーパラメータを調整したもの


# 振り返り
- 今回は他の参加者とは違い、すべての欠損値を補完して予測値を算出したが無理に補完するよりも欠損値を扱えるlightgbmなら精度は良かったかもしれない。  
- アンサンブルをする際にもっと特徴の異なるモデルを使った方が良かったかもしれない。(boosting_typeの変更など)
- さらに上位を狙うなら特徴量エンジニアリングの部分でまだできたことがあったかも。  
しかしながら、今回のコンペティションで特徴量エンジニアリングの大切さを学んだ。
- 次回はもっと深層学習の知識をつけて臨みたい。